{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVj1OJ2-LBwy"
      },
      "source": [
        "# Framework 2 - Embedding Level Fusion - Two Architecture \n",
        "\n",
        "\n",
        "\n",
        "1.   Params \"atten_mat\": True => Switch (SentiEmb X SentiAttenEmb + AggrEmb X AggrAttenEmb + Roberta InputEmb) -> CLS\n",
        "\n",
        "2.   Params \"atten_mat\": False => Switch (SentiEmb + AggrEmb + Roberta InputEmb) -> CLS\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1Ur7RsJLBwz"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tmpw4NWOkbL",
        "outputId": "fdf0b897-5d4b-4a6f-8e5f-c55da76ef0c2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wkXPTOQLBwz",
        "outputId": "0ee25a86-1bfd-48fb-8051-737837d86649"
      },
      "outputs": [],
      "source": [
        "import spacy.cli\n",
        "\n",
        "spacy.cli.download(\"en_core_web_lg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHNVOeS2LBwz",
        "outputId": "9c14b1d8-c8a9-49ce-b965-1465b9aeee51"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewkg-oiJLBw0"
      },
      "source": [
        "## Custom Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S42KV_x8LBw0",
        "outputId": "7845b1d0-a1bc-4e49-e329-15666d56b87a"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import PreTrainedTokenizer\n",
        "\n",
        "import nltk \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import numpy as np\n",
        "\n",
        "import spacy\n",
        "\n",
        "class EncodedDataset(Dataset):\n",
        "\n",
        "  def __init__(self, input_sents: List[str], \n",
        "                input_labels: List[int], \n",
        "                input_modifers:List[List], \n",
        "                tokenizer: PreTrainedTokenizer,\n",
        "                max_sequence_length: int = None, \n",
        "                max_targets: int = 5):\n",
        "      \n",
        "    self.input_sents = input_sents\n",
        "    self.input_labels = input_labels\n",
        "    self.input_modifers = input_modifers\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_sequence_length = max_sequence_length\n",
        "    self.max_targets = max_targets\n",
        "    # self.min_sequence_length = min_sequence_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_sents) \n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      \n",
        "    text = self.input_sents[index]\n",
        "    modifers = self.input_modifers[index]\n",
        "    label = self.input_labels[index]\n",
        "\n",
        "    # If we are doing some preprocessing\n",
        "    # preprocessor = PreProcess()\n",
        "\n",
        "    # senti_token = self.senti_tokenizer(text, padding='max_length', max_length= self.max_sequence_length, truncation=True)\n",
        "\n",
        "    # senti_input_ids, senti_mask_ids = torch.tensor(senti_token['input_ids']), torch.tensor(senti_token['attention_mask'])\n",
        "\n",
        "    token = self.tokenizer(text, padding='max_length', max_length= self.max_sequence_length, truncation=True)\n",
        "\n",
        "    input_ids, mask_ids = torch.tensor(token['input_ids']), torch.tensor(token['attention_mask'])\n",
        "\n",
        "    return input_ids, mask_ids, modifers, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BerNmFdLBw1",
        "outputId": "869d010d-e116-461d-8b66-0b6450e521e8"
      },
      "outputs": [],
      "source": [
        "target_extractor = ExtractTargets(is_nn=True, is_subphr=True, is_tar=True)\n",
        "\n",
        "targets = target_extractor.fit(\"The three different ways to become a brain dead zombie are, a virus, radiation and Chrisitanity.\")\n",
        "targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjlrZx0ULBw2"
      },
      "source": [
        "## Custom RoBERTa Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81vGjuJELBw2"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import Softmax, CrossEntropyLoss, MSELoss\n",
        "\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
        "\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "\n",
        "from collections import namedtuple\n",
        " \n",
        "class FeatureSwitchHead(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "      super().__init__()\n",
        "      self.dense = nn.Linear(config.hidden_size*(config.num_features+1), config.hidden_size*(config.num_features+1))\n",
        "      self.dropout = nn.Dropout(config.classifier_dropout)\n",
        "      self.out_proj = nn.Linear(config.hidden_size*(config.num_features+1), config.hidden_size)\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "      \n",
        "      x = features\n",
        "      x = self.dropout(x)\n",
        "      x = self.dense(x)\n",
        "      x = torch.tanh(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.out_proj(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "class RobertaHateClassificationHead(torch.nn.Module):\n",
        "  \n",
        "    def __init__(self, config):\n",
        "      super().__init__()\n",
        "      self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "      self.dropout = nn.Dropout(config.classifier_dropout)\n",
        "      self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "      x = features \n",
        "      x = self.dropout(x)\n",
        "      x = self.dense(x)\n",
        "      x = torch.tanh(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.out_proj(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "class RobertaForHateClassification(RobertaForSequenceClassification):\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config, feature_config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.soft_max = Softmax(dim=1)\n",
        "        self.switch_layer = FeatureSwitchHead(feature_config)\n",
        "        self.cls_layer = RobertaHateClassificationHead(feature_config)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "        aux_features=None\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
        "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        switch_output = torch.cat((outputs[0][:, 0, :], aux_features),1)\n",
        "\n",
        "        sequence_output = self.switch_layer(switch_output)\n",
        "\n",
        "        logits = self.cls_layer(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        softmax_logits = self.soft_max(logits)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (softmax_logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=softmax_logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_6xrX0xLBw3"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from sklearn.metrics import precision_score, \\\n",
        "    recall_score, confusion_matrix, classification_report, \\\n",
        "    accuracy_score, f1_score\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler\n",
        "\n",
        "def train(model, train_data, train_labels, train_modifers, val_data, val_labels, val_modifers, \n",
        "          senti_model, aggre_model, tokenizer, params):\n",
        "\n",
        "    train = EncodedDataset(input_sents=train_data, \n",
        "                    input_labels=train_labels, \n",
        "                    input_modifers=train_modifers,  \n",
        "                    tokenizer=tokenizer, \n",
        "                    max_sequence_length=params.max_sequence_length)\n",
        "    \n",
        "    val =  EncodedDataset(input_sents=val_data, \n",
        "                    input_labels=val_labels, \n",
        "                    input_modifers=val_modifers, \n",
        "                    tokenizer=tokenizer, \n",
        "                    max_sequence_length=params.max_sequence_length)\n",
        "\n",
        "    train_dataloader = DataLoader(train, batch_size=params.train_batch_size, shuffle=True)\n",
        "\n",
        "    val_dataloader = DataLoader(val, batch_size=params.val_batch_size)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=params.learning_rate)\n",
        "    \n",
        "    earlystop_epochs = 3 # 3 consecutive epochs without validation acc increase\n",
        "\n",
        "    save_dir = \"/content/drive/MyDrive/HateSpeechDet/\"\n",
        "\n",
        "    best_validation_accuracy = 1e-5\n",
        "    without_progress = 0\n",
        "    model.zero_grad()\n",
        "    for epoch_num in range(params.epochs):\n",
        "\n",
        "      total_acc_train = 0\n",
        "      total_loss_train = 0\n",
        "      predictions = []\n",
        "      y_true = []\n",
        "\n",
        "      with tqdm(train_dataloader, desc=\"Training\") as loop:\n",
        "\n",
        "        for train_input, train_mask, train_modifers, train_label in loop:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            train_input = train_input.to(device)\n",
        "            train_mask = train_mask.to(device)\n",
        "            # print(train_modifers)\n",
        "            # train_modifers = train_modifers.to(device)\n",
        "            train_label = train_label.to(device)\n",
        "\n",
        "            # input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "            # print(len(output.hidden_states))\n",
        "\n",
        "            aggr_output = aggre_model(input_ids=train_input, attention_mask=train_mask, output_hidden_states = True, output_attentions=True)\n",
        "\n",
        "            senti_output = senti_model(input_ids=train_input, attention_mask=train_mask, output_hidden_states = True, output_attentions=True)\n",
        "            \n",
        "            # Potential Update: Consider other layer heads (Sum/mean or specific) instead last one (-1)\n",
        "            if params.atten_mat:\n",
        "              aggr_att_scores = torch.sum(aggr_output.attentions[-1][0:,-1],axis=1)\n",
        "\n",
        "              aggr_mat = aggr_output.hidden_states[-1]*aggr_att_scores.unsqueeze(2)\n",
        "\n",
        "              senti_att_scores = torch.sum(senti_output.attentions[-1][0:,-1],axis=1)\n",
        "\n",
        "              senti_mat = senti_output.hidden_states[-1]*senti_att_scores.unsqueeze(2)\n",
        "\n",
        "            else:\n",
        "              aggr_mat = aggr_output.hidden_states[-1]\n",
        "\n",
        "              senti_mat = senti_output.hidden_states[-1]\n",
        "\n",
        "            aux_features = torch.cat((aggr_mat[:, 0, :],senti_mat[:, 0, :]), 1)\n",
        "\n",
        "            output = model(input_ids=train_input,\n",
        "                          attention_mask=train_mask,\n",
        "                          labels=train_label,\n",
        "                          aux_features=aux_features)\n",
        "            \n",
        "            loss, logits = output[\"loss\"], output[\"logits\"]\n",
        "            # print(\"Loss is:\" , model(texts, attention_mask=masks, labels=labels))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "      \n",
        "            \n",
        "            total_loss_train += loss.item()\n",
        "            acc = (logits.argmax(dim=1) == train_label).sum().item()\n",
        "            total_acc_train += acc\n",
        "\n",
        "            loop.set_postfix(loss=loss.item(), acc=acc/len(train_input))\n",
        "        \n",
        "        total_acc_val = 0\n",
        "        total_loss_val = 0\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for val_input, val_mask, val_modifers, val_label in val_dataloader:\n",
        "                \n",
        "                val_input = val_input.to(device)\n",
        "                val_mask = val_mask.to(device)\n",
        "                \n",
        "                # val_modifers = val_modifers.to(device)\n",
        "                val_label = val_label.to(device)\n",
        "\n",
        "                aggr_output = aggre_model(input_ids=val_input, attention_mask=val_mask, output_hidden_states = True, output_attentions=True)\n",
        "\n",
        "                senti_output = senti_model(input_ids=val_input, attention_mask=val_mask, output_hidden_states = True, output_attentions=True)\n",
        "\n",
        "                if params.atten_mat:\n",
        "                  aggr_att_scores = torch.sum(aggr_output.attentions[-1][0:,-1],axis=1)\n",
        "\n",
        "                  aggr_mat = aggr_output.hidden_states[-1]*aggr_att_scores.unsqueeze(2)\n",
        "\n",
        "                  senti_att_scores = torch.sum(senti_output.attentions[-1][0:,-1],axis=1)\n",
        "\n",
        "                  senti_mat = senti_output.hidden_states[-1]*senti_att_scores.unsqueeze(2)\n",
        "                \n",
        "                else:\n",
        "\n",
        "                  aggr_mat = aggr_output.hidden_states[-1]\n",
        "\n",
        "                  senti_mat = senti_output.hidden_states[-1]\n",
        "\n",
        "                aux_features = torch.cat((aggr_mat[:, 0, :],senti_mat[:, 0, :]), 1)\n",
        "\n",
        "                output = model(input_ids=val_input,\n",
        "                              attention_mask=val_mask,\n",
        "                              labels=val_label,\n",
        "                              aux_features=aux_features)\n",
        "\n",
        "                loss, logits = output[\"loss\"], output[\"logits\"]\n",
        "                \n",
        "                acc = (logits.argmax(dim=1) == val_label).sum().item()\n",
        "\n",
        "                total_acc_val += acc\n",
        "\n",
        "                predictions.extend(logits.argmax(dim=1).detach().cpu().numpy())\n",
        "\n",
        "                y_true.extend(val_label.detach().cpu().numpy())\n",
        "        \n",
        "        print(\n",
        "            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
        "            | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
        "            | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
        "            | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
        "        print(\"CLassification Report: \", classification_report(y_true,predictions))\n",
        "        \n",
        "        ## Early Stopping Criteria \n",
        "        if total_acc_val > best_validation_accuracy:\n",
        "\n",
        "          without_progress = 0 \n",
        "\n",
        "          best_validation_accuracy = total_acc_val\n",
        "\n",
        "          model_to_save = model\n",
        "          \n",
        "          fname = \"best-model_\" + params.dataset_name+\"_\"+str(epoch_num)+\".pt\"\n",
        "          torch.save(model_to_save.state_dict(), os.path.join(save_dir, fname))\n",
        "\n",
        "        else:\n",
        "\n",
        "          without_progress +=1\n",
        "        \n",
        "        if without_progress >= earlystop_epochs:\n",
        "          \n",
        "          print(\"Early stopping.....\")\n",
        "\n",
        "          print(\"Saving model: \", fname)\n",
        "\n",
        "          break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "bd14c11f9eac45ddafe446bed0594ea8",
            "3963b1e60f17491c92cd4f8d2e2f0389",
            "345a4af0a6fc4a3eba06206d53c90bed",
            "99eea3bc55ac40ea884f3ead8e36f1e2",
            "a308811b4d3c4eadba93e047a691a5cf",
            "503c745651204648b48b97cd126ba1eb",
            "ea5a17d27ff445078c5e7194d4ceb8b0",
            "8cd4de2cc4ce4781a99acc2fd29406b2",
            "62d5642c2be44138a966758b01f66786",
            "73c697e657af4985887044d5e6a88550",
            "d15fd4dbf02840d59a90cb04cded1708",
            "16bf91a72a0348a3854bd304b0288897",
            "d42438d007e9422b89d5cd76af23f2fb",
            "91a635b011024ab096bac74300a12e19",
            "fce606cf525c4e0ea1d297c27bc55ec6",
            "42805e98382342c180e7693bbc4db34a",
            "a8139911f608498d8eb7bea4c6928917",
            "ef5ab85cc7b04cc2820058106ea20457",
            "66736351dc5045f69bf8ae1c4fc19e3e",
            "4ce43b5f00cb473a8fc3253468176fdd",
            "92bfa33178584f81a9e05c48de6181d9",
            "64288e8235fa47b9bce4bcbb8c9d6d71",
            "a8eaf9ddee82423099c931ca3f01eab2",
            "f2153ecd866e4df29872aa350b65cead",
            "0eb04eea8fe54c17ae70810a7eb2dcb4",
            "04a9b7be897f414092f51121159aee36",
            "44ddad4624274f40a64d699259f7bc92",
            "fc6eba925743486ba9e7ec6049a917a7",
            "12cd45c983f7443f8c81a789cf96ee3a",
            "36c62fc6bee34b90b87384ce4e5a9b82",
            "e376999f964d42b98bd4b11a4097f355",
            "51888b5c696547389ca13e75413b4bf7",
            "060449a89093455395ee671ff20761fd",
            "20fad68abd8045889b8b6dedf0fc177a",
            "859e02e8ebd947cf925f6e8b75c5f0fc",
            "7539b1d4ca914bc1912f636e2babe5ea",
            "7067bbe491a64ee68e3ccfc8ab602331",
            "7664961089db4b098b3f0df33e27da5e",
            "4fd094f6a5a74418aa2c5d228172cc5b",
            "17500dca6ad14bb99a9c41542dac6ede",
            "6a4acbc796da4e079e406c77689da02c",
            "cdfdfc9b0e614634b27770f9fbe8fc4c",
            "dc095fee65824698bd5e31e1e182713f",
            "51919f080b824d908ad12d6ae103b4b3"
          ]
        },
        "id": "ZTBqtHcknSF9",
        "outputId": "d75cc56c-44c4-4fb9-f8fc-73c47b343b56"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "torch.cuda.synchronize()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "aggr_task='offensive'\n",
        "aggr_MODEL = f\"cardiffnlp/twitter-roberta-base-{aggr_task}\"\n",
        "\n",
        "# aggr_tokenizer = AutoTokenizer.from_pretrained(aggr_MODEL)\n",
        "aggr_model = AutoModelForSequenceClassification.from_pretrained(aggr_MODEL).to(device)\n",
        "# aggr_model.to(device=device)\n",
        "\n",
        "latest_task='sentiment-latest'\n",
        "sentiment_MODEL = f\"cardiffnlp/twitter-roberta-base-{latest_task}\"#This is a roBERTa-base model trained on ~58M\n",
        "# senti_tokenizer = AutoTokenizer.from_pretrained(sentiment_MODEL)\n",
        "senti_model = AutoModelForSequenceClassification.from_pretrained(sentiment_MODEL).to(device)\n",
        "\n",
        "\n",
        "train_frame = pd.read_csv(\"/content/drive/MyDrive/HateSpeechDet/EXP_WhiteSup_Train.csv\")\n",
        "test_frame = pd.read_csv(\"/content/drive/MyDrive/HateSpeechDet/EXP_FoxNews_Test.csv\")\n",
        "\n",
        "\n",
        "# full_model = BertClassifier(model_name=\"bert-base-uncased\",\n",
        "#                        sentiment_model=senti_model, \n",
        "#                        sentiment_tokenizer=senti_tokenizer, \n",
        "#                        aggression_model=aggr_model, \n",
        "#                        aggression_tokenizer=aggr_tokenizer, \n",
        "#                        dropout=0.5, \n",
        "#                        use_sentiment=True, \n",
        "#                        use_aggression=False, \n",
        "#                        use_modifiers=False)\n",
        "\n",
        "# full_model.to(device=device)\n",
        "# # full_model.load_state_dict(torch.load(\"Models/best-model_ConvAbuse_4.pt\"))\n",
        "# # Parameters\n",
        "# num_epochs = 5\n",
        "# lr = 1e-5\n",
        "# train_batch_size = 32\n",
        "# val_batch_size = 16\n",
        "# max_len = 512\n",
        "\n",
        "# train_frame = pd.read_csv(\"LargeDatasets/Synthetic_train.csv\")\n",
        "# # test_frame = pd.read_csv(\"IMP_OLIDv1.0_testing_SentiAgJoined_Modifiers.csv\")\n",
        "# test_frame = pd.read_csv(\"FoxNews_Test_Modifiers.csv\")\n",
        "\n",
        "# train_mods = [\" \".join(eval(x)) for x in train_frame['overall_modifiers'].values.tolist()]\n",
        "# test_mods = [\" \".join(eval(x)) for x in test_frame['overall_modifiers'].values.tolist()]\n",
        "\n",
        "# # n = len(train_frame.groupby('label').get_group(1))\n",
        "# # x = train_frame.groupby('label').get_group(0).sample(n=n)\n",
        "# # y = train_frame.groupby('label').get_group(1).sample(frac=1)\n",
        "# # train_frame1 = pd.concat([x,y])\n",
        "# # train_frame1 = train_frame1.sample(frac=1)\n",
        "# # print(n)\n",
        "\n",
        "class_weights = list(1- train_frame.groupby('label').count()['text'].values/(sum(train_frame.groupby('label').count()['text'].values)))\n",
        "class_weights = torch.FloatTensor(class_weights)\n",
        "\n",
        "# n = len(test_frame.groupby('label').get_group(1))\n",
        "# print(n)\n",
        "# x = test_frame.groupby('label').get_group(0).sample(n=n)\n",
        "# y = test_frame.groupby('label').get_group(1).sample(frac=1)\n",
        "# test_frame1 = pd.concat([x,y])\n",
        "# test_frame1 = test_frame1.sample(frac=1)\n",
        "\n",
        "\n",
        "# # full_model.load_state_dict(torch.load(\"Models/best-model_FoxNews_0.pt\"))\n",
        "\n",
        "# # train_frame = pd.read_csv(\"/content/drive/MyDrive/HateSpeechDet/EXP_WhiteSup_Train.csv\")\n",
        "# # test_frame = pd.read_csv(\"/content/drive/MyDrive/HateSpeechDet/EXP_WhiteSup_Test.csv\")\n",
        "\n",
        "# # train_frame = pd.read_csv(\"Data_Final/EXP_FoxNews_Train.csv\")\n",
        "# # # imp_train = pd.read_csv(\"Data_Final/IMP_FoxNews_Train.csv\")\n",
        "# # # train_frame = pd.concat([exp_train,imp_train])\n",
        "# # train_frame = train_frame.sample(frac=1)\n",
        "# # train_frame.loc[train_frame[\"label\"] >= 1, \"label\"] = 1\n",
        "\n",
        "# # test_frame =  pd.read_csv(\"Data_Final/EXP_FoxNews_Test.csv\")\n",
        "# # # imp_test = pd.read_csv(\"Data_Final/IMP_ConvAbuseEMNLPfull_Test.csv\")\n",
        "# # # test_frame = pd.concat([exp_test,imp_test])\n",
        "# # test_frame = test_frame.sample(frac=1)\n",
        "# # test_frame.loc[test_frame[\"label\"] >= 1, \"label\"] = 1\n",
        "\n",
        "from types import SimpleNamespace\n",
        "\n",
        "params = {\"max_sequence_length\": 512, \n",
        "          \"learning_rate\" : 1e-5, \n",
        "          \"train_batch_size\" : 2, \n",
        "          \"val_batch_size\" : 16, \n",
        "          \"epochs\" : 5, \n",
        "          \"device\" : device,\n",
        "          \"dataset_name\" : \"WhiteSupA2\",\n",
        "          \"class_weights\" : class_weights,\n",
        "          \"hidden_size\" : 768,\n",
        "          \"num_features\" : 2,\n",
        "          \"num_labels\": 2,\n",
        "          \"classifier_dropout\" : 0.5, \n",
        "          \"atten_mat\": False}\n",
        "\n",
        "params = SimpleNamespace(**params)\n",
        "\n",
        "model_name = \"roberta-base\"\n",
        "\n",
        "base_model = RobertaForHateClassification.from_pretrained(model_name, feature_config=params).to(device)\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "\n",
        "train(model=base_model,\n",
        "      train_data=train_frame['text'].values.tolist(), \n",
        "      train_labels=train_frame['label'].values.tolist(), \n",
        "      train_modifers=np.zeros(len(train_frame)).tolist(), \n",
        "      val_data=test_frame['text'].values.tolist(), \n",
        "      val_labels=test_frame['label'].values.tolist(), \n",
        "      val_modifers=np.zeros(len(test_frame)).tolist(), \n",
        "      tokenizer=tokenizer, \n",
        "      senti_model = senti_model,\n",
        "      aggre_model = aggr_model,\n",
        "      params=params)\n",
        "# # evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pr-ButdcsTQT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH5ZYzFZdM36"
      },
      "source": [
        "# Framework 2 - Feature Importance - Embedding Fusion  \n",
        "\n",
        "1.   Switch (SentiAttenEmb + AggrAttenEmb) X  Roberta InputEmb -> CLS\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1zj9DEBdM37"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoMxqYX-dM39",
        "outputId": "0ee25a86-1bfd-48fb-8051-737837d86649"
      },
      "outputs": [],
      "source": [
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_lg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOnXMpGldM39",
        "outputId": "9c14b1d8-c8a9-49ce-b965-1465b9aeee51"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j13jK4opdM3-"
      },
      "source": [
        "## Custom Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XewlfDrrdM3-",
        "outputId": "e8908a60-72ad-49b7-f6c0-dbea17bdd90a"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import PreTrainedTokenizer\n",
        "\n",
        "import nltk \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import numpy as np\n",
        "\n",
        "import spacy\n",
        "\n",
        "\n",
        "class EncodedDataset(Dataset):\n",
        "\n",
        "  def __init__(self, input_sents: List[str], \n",
        "                input_labels: List[int], \n",
        "                input_modifers:List[List], \n",
        "                tokenizer: PreTrainedTokenizer,\n",
        "                max_sequence_length: int = None, \n",
        "                max_targets: int = 5):\n",
        "      \n",
        "    self.input_sents = input_sents\n",
        "    self.input_labels = input_labels\n",
        "    self.input_modifers = input_modifers\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_sequence_length = max_sequence_length\n",
        "    self.max_targets = max_targets\n",
        "    # self.min_sequence_length = min_sequence_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_sents) \n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      \n",
        "    text = self.input_sents[index]\n",
        "    modifers = self.input_modifers[index]\n",
        "    label = self.input_labels[index]\n",
        "\n",
        "    # If we are doing some preprocessing\n",
        "    # preprocessor = PreProcess()\n",
        "\n",
        "    # senti_token = self.senti_tokenizer(text, padding='max_length', max_length= self.max_sequence_length, truncation=True)\n",
        "\n",
        "    # senti_input_ids, senti_mask_ids = torch.tensor(senti_token['input_ids']), torch.tensor(senti_token['attention_mask'])\n",
        "\n",
        "    token = self.tokenizer(text, padding='max_length', max_length= self.max_sequence_length, truncation=True)\n",
        "\n",
        "    input_ids, mask_ids = torch.tensor(token['input_ids']), torch.tensor(token['attention_mask'])\n",
        "\n",
        "    return input_ids, mask_ids, modifers, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE-i3kBNdM4A"
      },
      "source": [
        "## Custom RoBERTa Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRCr09_-dM4A"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import Softmax, CrossEntropyLoss, MSELoss\n",
        "\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
        "\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "\n",
        "from collections import namedtuple\n",
        " \n",
        "class FeatureSwitchHead(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "      super().__init__()\n",
        "      self.dense = nn.Linear(config.max_sequence_length*config.num_features, config.max_sequence_length*config.num_features)\n",
        "      self.dropout = nn.Dropout(config.classifier_dropout)\n",
        "      self.out_proj = nn.Linear(config.max_sequence_length*config.num_features, config.max_sequence_length)\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "      \n",
        "      x = features\n",
        "      x = self.dropout(x)\n",
        "      x = self.dense(x)\n",
        "      x = torch.tanh(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.out_proj(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "class RobertaHateClassificationHead(torch.nn.Module):\n",
        "  \n",
        "    def __init__(self, config):\n",
        "      super().__init__()\n",
        "      self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "      self.dropout = nn.Dropout(config.classifier_dropout)\n",
        "      self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "      x = features[:, 0, :] \n",
        "      x = self.dropout(x)\n",
        "      x = self.dense(x)\n",
        "      x = torch.tanh(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.out_proj(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "class RobertaForHateClassification(RobertaForSequenceClassification):\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config, feature_config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.soft_max = Softmax(dim=1)\n",
        "        self.switch_layer = FeatureSwitchHead(feature_config)\n",
        "        self.cls_layer = RobertaHateClassificationHead(feature_config)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "        aux_attention=None,\n",
        "        class_weights=None\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
        "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "\n",
        "        switch_attn = self.switch_layer(aux_attention)\n",
        "\n",
        "        sequence_output = outputs[0]*switch_attn.unsqueeze(2)\n",
        "\n",
        "        logits = self.cls_layer(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss(weight=class_weights)\n",
        "                loss_fct = loss_fct.to(device)\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        softmax_logits = self.soft_max(logits)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (softmax_logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=softmax_logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8nGjTU_dM4B"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam, lr_scheduler\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from sklearn.metrics import precision_score, \\\n",
        "    recall_score, confusion_matrix, classification_report, \\\n",
        "    accuracy_score, f1_score\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler\n",
        "\n",
        "def train(model, train_data, train_labels, train_modifers, val_data, val_labels, val_modifers, \n",
        "          senti_model, aggre_model, tokenizer, params):\n",
        "    accumulation_steps = 4\n",
        "\n",
        "    train = EncodedDataset(input_sents=train_data, \n",
        "                    input_labels=train_labels, \n",
        "                    input_modifers=train_modifers,  \n",
        "                    tokenizer=tokenizer, \n",
        "                    max_sequence_length=params.max_sequence_length)\n",
        "    \n",
        "    val =  EncodedDataset(input_sents=val_data, \n",
        "                    input_labels=val_labels, \n",
        "                    input_modifers=val_modifers, \n",
        "                    tokenizer=tokenizer, \n",
        "                    max_sequence_length=params.max_sequence_length)\n",
        "\n",
        "    train_dataloader = DataLoader(train, batch_size=params.train_batch_size, shuffle=True)\n",
        "\n",
        "    val_dataloader = DataLoader(val, batch_size=params.val_batch_size)\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=params.learning_rate)\n",
        "    \n",
        "    earlystop_epochs = 3 # 3 consecutive epochs without validation acc increase\n",
        "\n",
        "    save_dir = \"Models/Roberta-Senti+Agg1/\"\n",
        "\n",
        "    best_validation_accuracy = 1e-5\n",
        "    best_validation_accuracy1 = 1e-5\n",
        "    without_progress = 0\n",
        "    model.zero_grad()\n",
        "    for epoch_num in range(params.epochs):\n",
        "\n",
        "      total_acc_train = 0\n",
        "      total_loss_train = 0\n",
        "      predictions = []\n",
        "      y_true = []\n",
        "      c=0\n",
        "\n",
        "      with tqdm(train_dataloader, desc=\"Training\") as loop:\n",
        "\n",
        "        for train_input, train_mask, train_modifers, train_label in loop:\n",
        "            model.train()\n",
        "            c+=1\n",
        "            train_input = train_input.to(device)\n",
        "            train_mask = train_mask.to(device)\n",
        "            train_label = train_label.to(device)\n",
        "\n",
        "            aggr_output = aggre_model(input_ids=train_input, attention_mask=train_mask, output_hidden_states = True, output_attentions=True)\n",
        "\n",
        "            senti_output = senti_model(input_ids=train_input, attention_mask=train_mask, output_hidden_states = True, output_attentions=True)\n",
        "            \n",
        "            # Potential Update: Consider other layer heads (Sum/mean or specific) instead last one (-1)\n",
        "            \n",
        "            aggr_att_scores = torch.sum(aggr_output.attentions[-1][0:,-1],axis=1)\n",
        "\n",
        "            senti_att_scores = torch.sum(senti_output.attentions[-1][0:,-1],axis=1)\n",
        "\n",
        "            attn_mat = torch.cat((aggr_att_scores,senti_att_scores), 1)\n",
        "\n",
        "            output = model(input_ids=train_input,\n",
        "                          attention_mask=train_mask,\n",
        "                          labels=train_label,\n",
        "                          aux_attention=attn_mat,class_weights=params.class_weights)\n",
        "            \n",
        "            loss, logits = output[\"loss\"], output[\"logits\"]\n",
        "            l2_reg = torch.tensor(0., requires_grad=True)\n",
        "            for param in model.parameters():\n",
        "                l2_reg = l2_reg + torch.norm(param, 2)\n",
        "            loss = loss + (l2_reg * 0.001)\n",
        "            loss.backward()\n",
        "            if (c + 1) % accumulation_steps == 0:\n",
        "              optimizer.step()\n",
        "              optimizer.zero_grad()      \n",
        "            \n",
        "            total_loss_train += loss.item()\n",
        "            acc = (logits.argmax(dim=1) == train_label).sum().item()\n",
        "            total_acc_train += acc\n",
        "\n",
        "            loop.set_postfix(loss=loss.item(), acc=acc/len(train_input))\n",
        "\n",
        "        total_acc_val = 0\n",
        "        total_loss_val = 0\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for val_input, val_mask, val_modifers, val_label in val_dataloader:\n",
        "                \n",
        "                val_input = val_input.to(device)\n",
        "                val_mask = val_mask.to(device)\n",
        "                \n",
        "                val_label = val_label.to(device)\n",
        "\n",
        "                aggr_output = aggre_model(input_ids=val_input, attention_mask=val_mask, output_hidden_states = True, output_attentions=True)\n",
        "\n",
        "                senti_output = senti_model(input_ids=val_input, attention_mask=val_mask, output_hidden_states = True, output_attentions=True)\n",
        "\n",
        "                aggr_att_scores = torch.sum(aggr_output.attentions[-1][0:,-1],axis=1)\n",
        "\n",
        "                senti_att_scores = torch.sum(senti_output.attentions[-1][0:,-1],axis=1)\n",
        "\n",
        "                attn_mat = torch.cat((aggr_att_scores,senti_att_scores), 1)\n",
        "\n",
        "                output = model(input_ids=val_input,\n",
        "                              attention_mask=val_mask,\n",
        "                              labels=val_label,\n",
        "                              aux_attention=attn_mat)\n",
        "\n",
        "                loss, logits = output[\"loss\"], output[\"logits\"]\n",
        "                \n",
        "                acc = (logits.argmax(dim=1) == val_label).sum().item()\n",
        "\n",
        "                total_acc_val += acc\n",
        "\n",
        "                predictions.extend(logits.argmax(dim=1).detach().cpu().numpy())\n",
        "\n",
        "                y_true.extend(val_label.detach().cpu().numpy())\n",
        "        \n",
        "        print(\n",
        "            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
        "            | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
        "            | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
        "            | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
        "        print(\"CLassification Report: \", classification_report(y_true,predictions))\n",
        "        \n",
        "        ## Early Stopping Criteria \n",
        "        temp = classification_report(y_true, predictions,output_dict=True)\n",
        "        macro = pd.DataFrame(temp)['1']['f1-score']\n",
        "        m1 = pd.DataFrame(temp)['macro avg']['f1-score']\n",
        "        if macro >= best_validation_accuracy or m1>=best_validation_accuracy1:\n",
        "\n",
        "          without_progress = 0 \n",
        "\n",
        "          if(macro>=best_validation_accuracy):\n",
        "            best_validation_accuracy = macro\n",
        "          \n",
        "          if(m1>=best_validation_accuracy1):\n",
        "            best_validation_accuracy1 = m1\n",
        "\n",
        "          model_to_save = model\n",
        "          \n",
        "          fname = \"best-model_\" + params.dataset_name+\"_\"+str(epoch_num+1)+\".pt\"\n",
        "          torch.save(model_to_save.state_dict(), os.path.join(save_dir, fname))\n",
        "          print(\"Saved at \",os.path.join(save_dir, fname))\n",
        "\n",
        "        else:\n",
        "\n",
        "          without_progress +=1\n",
        "        \n",
        "        if without_progress >= earlystop_epochs:\n",
        "          \n",
        "          print(\"Early stopping.....\")\n",
        "\n",
        "          print(\"Saving model: \", fname)\n",
        "\n",
        "          break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bP3kKc6gdM4D"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from types import SimpleNamespace\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(26)\n",
        "\n",
        "#Replace with the test files\n",
        "test_files = ['C:/Users/psheth5/OneDrive - Arizona State University/HateSpeech Datasets/Dataset-01_24/GabHateCorpusannotations_Test_Modifiers.csv',\n",
        "'C:/Users/psheth5/OneDrive - Arizona State University/HateSpeech Datasets/Dataset-01_24/ICWSM18SALMINEN_Test_Modifiers.csv',\n",
        "'C:/Users/psheth5/OneDrive - Arizona State University/HateSpeech Datasets/Dataset-01_24/Reddit_Test_Modifiers.csv',\n",
        "'C:/Users/psheth5/OneDrive - Arizona State University/HateSpeech Datasets/Dataset-01_24/WikiDetox_Test_Modifiers.csv',\n",
        "'C:/Users/psheth5/OneDrive - Arizona State University/HateSpeech Datasets/Dataset-01_24/Twi-Red-You_test.csv']\n",
        "\n",
        "#Replace with the train files\n",
        "files = [\"C:/Users/psheth5/OneDrive - Arizona State University/HateSpeech Datasets/Dataset-01_24/HateEval/HateEval_train.csv\",\n",
        "'C:/Users/psheth5/OneDrive - Arizona State University/HateSpeech Datasets/Dataset-01_24/HateEval/migrants/migrants_train.csv',\n",
        "\"C:/Users/psheth5/OneDrive - Arizona State University/HateSpeech Datasets/Dataset-01_24/HateEval/lgbt/lgbt_train.csv\"]\n",
        "dataset_names = ['HateEval','HateEval_migrants','HateEval_lgbt']\n",
        "print(files)\n",
        "print(len(files))\n",
        "filenames = set()\n",
        "for f in range(1,len(files)):\n",
        "        if(dataset_names[f] not in filenames):\n",
        "            filenames.add(dataset_names[f])\n",
        "            aggr_task='offensive'\n",
        "            aggr_MODEL = f\"cardiffnlp/twitter-roberta-base-{aggr_task}\"\n",
        "\n",
        "            aggr_model = AutoModelForSequenceClassification.from_pretrained(aggr_MODEL).to(device)\n",
        "\n",
        "            latest_task='sentiment-latest'\n",
        "            sentiment_MODEL = f\"cardiffnlp/twitter-roberta-base-{latest_task}\"#This is a roBERTa-base model trained on ~58M\n",
        "            senti_model = AutoModelForSequenceClassification.from_pretrained(sentiment_MODEL).to(device)\n",
        "            \n",
        "            train_frame = pd.read_csv(files[f])\n",
        "            test_frame = pd.read_csv(\"C:/Users/psheth5/OneDrive - Arizona State University/HateSpeech Datasets/Dataset-01_24/GabHateCorpusannotations_Test_Modifiers_HB.csv\")\n",
        "            class_weights = compute_class_weight('balanced', classes=np.unique(train_frame['label']), y=train_frame['label'])\n",
        "            class_weights = torch.FloatTensor(class_weights)\n",
        "            params = {\"max_sequence_length\": 512, \n",
        "            \"learning_rate\" : 2e-5, \n",
        "            \"train_batch_size\" : 8, \n",
        "            \"val_batch_size\" : 16, \n",
        "            \"epochs\" : 15, \n",
        "            \"device\" : device,\n",
        "            \"dataset_name\" : dataset_names[f],\n",
        "            \"class_weights\" : class_weights,\n",
        "            \"hidden_size\" : 768,\n",
        "            \"num_features\" : 2,\n",
        "            \"num_labels\": 2,\n",
        "            \"classifier_dropout\" : 0.2\n",
        "            }\n",
        "            params = SimpleNamespace(**params)\n",
        "\n",
        "            model_name = \"roberta-base\"\n",
        "\n",
        "            base_model = RobertaForHateClassification.from_pretrained(model_name, feature_config=params).to(device)\n",
        "            tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "            train(model=base_model,\n",
        "            train_data=train_frame['text'].values.tolist(), \n",
        "            train_labels=train_frame['label'].values.tolist(), \n",
        "            train_modifers=np.zeros(len(train_frame)).tolist(), \n",
        "            val_data=test_frame['text'].values.tolist(), \n",
        "            val_labels=test_frame['label'].values.tolist(), \n",
        "            val_modifers=np.zeros(len(test_frame)).tolist(), \n",
        "            tokenizer=tokenizer, \n",
        "            senti_model = senti_model,\n",
        "            aggre_model = aggr_model,\n",
        "            params=params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, test_data, test_labels, \n",
        "          senti_tokenizer, aggre_tokenizer, test_modifiers, max_sequence_length, learning_rate, test_batch_size, device):\n",
        "    \n",
        "    test = EncodedDataset(input_sents=test_data, \n",
        "                    input_labels=test_labels, \n",
        "                    input_modifers=test_modifiers,  \n",
        "                    tokenizer=tokenizer, \n",
        "                    max_sequence_length=max_sequence_length)\n",
        "    \n",
        "\n",
        "    test_dataloader = DataLoader(test, batch_size=test_batch_size)\n",
        "\n",
        "\n",
        "    total_acc_test = 0\n",
        "    total_loss_test = 0\n",
        "    predictions = []\n",
        "    y_true = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():   \n",
        "      for test_input, test_mask, test_modifiers, test_label in test_dataloader:\n",
        "        test_input = test_input.to(device)\n",
        "        test_mask = test_mask.to(device)\n",
        "        \n",
        "        # val_modifers = val_modifers.to(device)\n",
        "        test_label = test_label.to(device)\n",
        "\n",
        "        aggr_output = aggr_model(input_ids=test_input, attention_mask=test_mask, output_hidden_states = True, output_attentions=True)\n",
        "\n",
        "        senti_output = senti_model(input_ids=test_input, attention_mask=test_mask, output_hidden_states = True, output_attentions=True)\n",
        "\n",
        "        aggr_att_scores = torch.sum(aggr_output.attentions[-1][0:,-1],axis=1)\n",
        "\n",
        "        senti_att_scores = torch.sum(senti_output.attentions[-1][0:,-1],axis=1)\n",
        "\n",
        "        attn_mat = torch.cat((aggr_att_scores,senti_att_scores), 1)\n",
        "\n",
        "        output = model(input_ids=test_input,\n",
        "                      attention_mask=test_mask,\n",
        "                      labels=test_label,\n",
        "                      aux_attention=attn_mat)\n",
        "\n",
        "        loss, logits = output[\"loss\"], output[\"logits\"]\n",
        "        \n",
        "        acc = (logits.argmax(dim=1) == test_label).sum().item()\n",
        "\n",
        "        predictions.extend(logits.argmax(dim=1).detach().cpu().numpy())\n",
        "\n",
        "        y_true.extend(test_label.detach().cpu().numpy())\n",
        "      return predictions,y_true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Load your model files here\n",
        "model_files = ['C:/Users/psheth5/OneDrive - Arizona State University/HateSpeech Datasets/Models/Roberta-Senti+Agg1/best-model_Reddit_1.pt']\n",
        "\n",
        "#Load your test files here\n",
        "files = ['C:/Users/psheth5/OneDrive - Arizona State University/HateSpeech Datasets/Dataset-01_24/GabHateCorpusannotations_Test_Modifiers_HB.csv',\n",
        "'C:/Users/psheth5/OneDrive - Arizona State University/HateSpeech Datasets/Dataset-01_24/ICWSM18SALMINEN_Test_Modifiers_HB.csv',\n",
        "'C:/Users/psheth5/OneDrive - Arizona State University/HateSpeech Datasets/Dataset-01_24/Reddit_Test_Modifiers_HB.csv',\n",
        "'C:/Users/psheth5/OneDrive - Arizona State University/HateSpeech Datasets/Dataset-01_24/WikiDetox_Test_Modifiers_HB.csv',\n",
        "'C:/Users/psheth5/OneDrive - Arizona State University/HateSpeech Datasets/Dataset-01_24/Twi-Red-You_Test_Modifiers_HB.csv']\n",
        "\n",
        "for f in model_files:\n",
        "    base_model.load_state_dict(torch.load(f))\n",
        "    print(f)\n",
        "    print(\"*\"*100)\n",
        "    for f1 in files:\n",
        "        test_frame =  pd.read_csv(f1)\n",
        "        preds,trues = evaluate(model=base_model, test_data = test_frame['text'].values.tolist(), test_labels=test_frame['label'].values.tolist(),\n",
        "        senti_tokenizer=senti_model, aggre_tokenizer=aggr_model,test_modifiers=np.zeros(len(test_frame)), max_sequence_length=512, learning_rate=1e-5, test_batch_size=8, device=device)\n",
        "        print(f1)\n",
        "        print(\"*\"*100)\n",
        "        print(classification_report(trues,preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "8a6511c335c26d02572349af57853ebfcc20500c776be641a45dee87cf591594"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
